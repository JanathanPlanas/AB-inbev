# BEES Data Engineering â€“ Breweries Pipeline

A data pipeline solution for the BEES/AB-InBev Data Engineering case. This project consumes data from the [Open Brewery DB API](https://www.openbrewerydb.org/), transforms it following the **Medallion Architecture**, and provides an aggregated analytical layer.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Project Structure](#project-structure)
- [Tech Stack](#tech-stack)
- [Getting Started](#getting-started)
- [Pipeline Layers](#pipeline-layers)
- [Orchestration](#orchestration)
- [Monitoring & Alerting](#monitoring--alerting)
- [Testing](#testing)
- [Design Decisions](#design-decisions)

## Overview

This pipeline fetches brewery data from a public API and processes it through three layers:

1. **Bronze (Raw)**: Raw data persisted as-is from the API
2. **Silver (Curated)**: Cleaned and transformed data in Parquet format, partitioned by location
3. **Gold (Aggregated)**: Analytical layer with brewery counts by type and location

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Open Brewery   â”‚â”€â”€â”€â”€â–¶â”‚     Bronze      â”‚â”€â”€â”€â”€â–¶â”‚     Silver      â”‚â”€â”€â”€â”€â–¶â”‚      Gold       â”‚
â”‚    DB API       â”‚     â”‚   (Raw JSON)    â”‚     â”‚   (Parquet)     â”‚     â”‚  (Aggregated)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚                        â”‚
                              â–¼                        â–¼                        â–¼
                        Native format           Partitioned by            Breweries per
                                               state/country             type & location
```

## Project Structure

```
AB-INBEV/
â”‚
â”œâ”€â”€ app/                     # Application entry points
â”‚
â”œâ”€â”€ config/                  # Configuration files (YAML, env templates)
â”‚
â”œâ”€â”€ data/                    # Data Lake (Medallion Architecture)
â”‚   â”œâ”€â”€ bronze/              # Raw data from API
â”‚   â”œâ”€â”€ silver/              # Transformed parquet files
â”‚   â””â”€â”€ gold/                # Aggregated analytical data
â”‚
â”œâ”€â”€ doc/                     # Documentation and case description
â”‚   â””â”€â”€ DE_Case_Atualizado.pdf
â”‚
â”œâ”€â”€ orchestration/           # Orchestration DAGs and workflows
â”‚   â””â”€â”€ dags/                # Airflow DAGs (or Mage pipelines)
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ clients/             # API clients (Open Brewery DB)
â”‚   â”œâ”€â”€ config/              # Config loaders and settings
â”‚   â”œâ”€â”€ io/                  # I/O operations (readers, writers)
â”‚   â”œâ”€â”€ pipelines/           # Pipeline definitions
â”‚   â”œâ”€â”€ transforms/          # Data transformations (bronzeâ†’silverâ†’gold)
â”‚   â””â”€â”€ utils/               # Utility functions and helpers
â”‚
â”œâ”€â”€ tests/                   # Unit and integration tests
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml       # Docker Compose for local execution
â”œâ”€â”€ Dockerfile               # Container definition
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md
```

## Tech Stack

| Component       | Technology                     |
|-----------------|--------------------------------|
| Language        | Python 3.11+                   |
| Data Processing | Pandas / PySpark               |
| Storage Format  | Parquet                        |
| Orchestration   | Apache Airflow / Mage          |
| Containerization| Docker & Docker Compose        |
| Testing         | pytest                         |
| Code Quality    | black, isort, flake8           |

## Getting Started

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- Make (optional)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/ab-inbev-breweries.git
   cd ab-inbev-breweries
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # or
   .\venv\Scripts\activate   # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run with Docker (recommended)**
   ```bash
   docker-compose up --build
   ```

### Running the Pipeline

```bash
# Run the full pipeline
python -m app.main

# Or run individual layers
python -m src.pipelines.bronze_pipeline
python -m src.pipelines.silver_pipeline
python -m src.pipelines.gold_pipeline
```

## Pipeline Layers

### Bronze Layer (Raw)

- Fetches all breweries from the Open Brewery DB API
- Handles pagination automatically
- Persists raw JSON data
- Includes metadata: ingestion timestamp, source URL

### Silver Layer (Curated)

- Converts JSON to Parquet (columnar format)
- **Partitioned by**: `country` and `state`
- Transformations applied:
  - Data type standardization
  - Null handling
  - Column renaming (snake_case)
  - Deduplication

### Gold Layer (Aggregated)

- Aggregated view: **quantity of breweries per type and location**
- Optimized for analytical queries
- Output schema:
  ```
  | country | state | brewery_type | brewery_count |
  ```

## Orchestration

The pipeline is orchestrated using **[Airflow/Mage]** with the following features:

- **Scheduling**: Daily execution at 00:00 UTC
- **Retries**: 3 attempts with exponential backoff
- **Error Handling**: Alerts on failure, graceful degradation
- **Dependencies**: Bronze â†’ Silver â†’ Gold (sequential)

```
[Extract API] â†’ [Load Bronze] â†’ [Transform Silver] â†’ [Aggregate Gold]
```

## Monitoring & Alerting

### Strategy

| Aspect             | Implementation                                      |
|--------------------|-----------------------------------------------------|
| Pipeline Failures  | Airflow alerts via email/Slack on task failure      |
| Data Quality       | Row count validation, schema checks, null monitoring|
| Latency            | SLA monitoring for each layer                       |
| Logging            | Structured logs with correlation IDs                |

### Data Quality Checks

- **Bronze**: API response validation, schema consistency
- **Silver**: Row count comparison, null percentage thresholds
- **Gold**: Aggregation integrity (sum validation)

### Recommended Tools (Production)

- **Observability**: Datadog, Grafana
- **Data Quality**: Great Expectations, dbt tests
- **Alerting**: PagerDuty, Slack webhooks

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/unit/test_api_client.py
```

### Test Coverage

- `src/clients/` - API client mocking and response handling
- `src/transforms/` - Transformation logic validation
- `src/pipelines/` - Integration tests for pipeline flow

## Design Decisions

| Decision                  | Rationale                                                  |
|---------------------------|------------------------------------------------------------|
| Parquet over Delta        | Simpler setup for local execution; Delta adds complexity   |
| Partition by state        | Balanced partition size; avoids small files problem        |
| Pandas over PySpark       | Dataset size (~8k rows) doesn't justify Spark overhead     |
| Airflow for orchestration | Industry standard; rich ecosystem; good for demonstration  |
| Docker Compose            | Easy local setup; reproducible environment                 |

### Trade-offs

1. **Local storage vs Cloud**: Used local filesystem for simplicity. In production, would use S3/GCS/ADLS.
2. **Batch vs Streaming**: Batch processing chosen as brewery data doesn't change frequently.
3. **Single node vs Distributed**: Pandas sufficient for current data volume; PySpark ready if needed.

## License

This project was created as part of a technical assessment for AB-InBev/BEES.

---

**Author**: [Your Name]  
**Date**: January 2025